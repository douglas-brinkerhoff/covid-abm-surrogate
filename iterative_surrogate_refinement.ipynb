{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook constructs an emulator for an agent based model using an ensemble of parameters.  It then performs MCMC sampling on it.  Some of these MCMC samples are fed back into the ABM to create better and targeted training data in the region of good fidelity to the data.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "\n",
    "import h5py  \n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils import data\n",
    "import json\n",
    "from sklearn.preprocessing import normalize\n",
    "from torch.utils.data import TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pandemic as pan\n",
    "from multiprocessing import Pool\n",
    "from data_tools import hdf5Dataset_init,normalize_sets,Emulator\n",
    "from scipy.stats import beta\n",
    "from scipy.special import gamma\n",
    "import pickle\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "keys = ['initial_infected_fraction', 'initial_removed_fraction', 'incubation_period', \n",
    "        'serial_interval', 'symptomatic_fraction', 'recovery_days', 'quarantine_days', \n",
    "        'days_indetectable', 'R0', 'contact_rate', 'npi_factor', 'contact_tracing_testing_rate', \n",
    "        'contact_tracing_quarantine_rate', 'contact_tracing_days', 'daily_testing_fraction', \n",
    "        'daily_testing_false_positive', 'daily_testing_false_negative', 'class_size_limit', \n",
    "        'contact_upscale_factor', 'friendship_contacts', 'academic_contacts', 'broad_social_contacts', \n",
    "        'department_environmental_contacts', 'broad_environmental_contacts', 'residential_neighbors', \n",
    "        'online_transition']\n",
    "\n",
    "keys_to_round = ['incubation_period', 'serial_interval', 'recovery_days',\n",
    "                 'quarantine_days', 'days_indetectable', 'contact_tracing_days',\n",
    "                 'class_size_limit', 'friendship_contacts', 'academic_contacts',\n",
    "                 'broad_social_contacts', 'department_environmental_contacts',\n",
    "                 'broad_environmental_contacts', 'residential_neighbors', 'online_transition']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the initial (large) ensemble of ABM model runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initset = hdf5Dataset_init('full_data.hdf5')   \n",
    "training, testing = initset.split_datasets(.1,split_type='parameter')\n",
    "training_,testing_,X_mean,X_std = normalize_sets(training,testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the COVID data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('datasets/testing.csv',skiprows=9)\n",
    "active = pd.read_csv('datasets/active.csv',skiprows=1)\n",
    "av = active.values[7:,2]\n",
    "r = np.linspace(0,len(av)-1,len(av))\n",
    "dd = np.vstack((r,av)).T\n",
    "ac = dd[np.invert(np.isnan(dd[:,1].astype(float)))]\n",
    "\n",
    "rapid_pcr_tests = np.nan_to_num(data.values[79:,2].astype(float))\n",
    "rapid_pcr_pos = np.nan_to_num(data.values[79:,4].astype(float))\n",
    "rapid_antigen_tests = np.nan_to_num(data.values[79:,3].astype(float))\n",
    "rapid_antigen_pos = np.nan_to_num(data.values[79:,5].astype(float))\n",
    "state_tests = np.nan_to_num(data.values[79:,6].astype(float))\n",
    "state_pos = np.nan_to_num(data.values[79:,8].astype(float))\n",
    "\n",
    "total_tests = rapid_pcr_tests + rapid_antigen_tests + state_tests\n",
    "total_pos = rapid_pcr_pos + rapid_antigen_pos + state_pos\n",
    "cum_tests = np.cumsum(total_tests)\n",
    "cum_pos = np.cumsum(total_pos)\n",
    "n_data = 194 - 90\n",
    "\n",
    "test_obs = torch.tensor(cum_tests,dtype=torch.float,device=device)[:n_data]\n",
    "pos_obs = torch.tensor(cum_pos,dtype=torch.float,device=device)[:n_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find minimum and maximum values for the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_b = 3\n",
    "beta_b = 3\n",
    "\n",
    "X = torch.stack([t[0] for t in training_])\n",
    "\n",
    "X_min = X.cpu().numpy().min(axis=0)-1e-3\n",
    "X_max = X.cpu().numpy().max(axis=0)+1e-3\n",
    "\n",
    "X_min = torch.tensor(X_min,dtype=torch.float32,device=device)\n",
    "X_max = torch.tensor(X_max,dtype=torch.float32,device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the log-posterior function for ABM sampling.  The alpha parameter is used to downweight the likelihood, since we know that the model is misspecified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "sigma_tes = 100\n",
    "sigma_pos = 10\n",
    "nu = 1.\n",
    "\n",
    "time = torch.linspace(1,n_data,n_data,device=device)\n",
    "def V(X):\n",
    "    _pred = 2**model(X) - 1.\n",
    "\n",
    "    test_pred = _pred[:n_data,2]\n",
    "    pos_pred = _pred[:n_data,1]\n",
    "    \n",
    "    r_test = (test_pred - test_obs)\n",
    "    r_pos = (pos_pred - pos_obs)\n",
    "    X_bar = (X - X_min)/(X_max - X_min)\n",
    "\n",
    "    sigma_tes_t = torch.sqrt(time)*sigma_tes\n",
    "    sigma_pos_t = torch.sqrt(time)*sigma_pos\n",
    "\n",
    "    L1a = torch.sum(np.log(gamma((nu+1)/2.)) - np.log(gamma(nu/2.)) - torch.log(np.sqrt(np.pi*nu)*sigma_tes_t) - (nu+1)/2.*torch.log(1 + 1./nu*(r_test**2/sigma_tes_t**2)))\n",
    "    L1b = torch.sum(np.log(gamma((nu+1)/2.)) - np.log(gamma(nu/2.)) - torch.log(np.sqrt(np.pi*nu)*sigma_pos_t) - (nu+1)/2.*torch.log(1 + 1./nu*(r_pos**2/sigma_pos_t**2)))\n",
    "    L2 = torch.sum((alpha_b-1)*torch.log(X_bar) + (beta_b-1)*torch.log(1-X_bar)) \n",
    "\n",
    "    return -(alpha*(L1a + L1b) + L2)\n",
    "\n",
    "def V_empirical(X,Y):\n",
    "    _pred = Y\n",
    "    #print(_pred)\n",
    "\n",
    "    test_pred = _pred[:n_data,2]\n",
    "    pos_pred = _pred[:n_data,1]\n",
    "    #print(U_pred.min())\n",
    "    r_test = (test_pred - test_obs)\n",
    "    r_pos = (pos_pred - pos_obs)\n",
    "    X_bar = (X - X_min)/(X_max - X_min)\n",
    "    sigma_tes_t = torch.sqrt(time)*sigma_tes\n",
    "    sigma_pos_t = torch.sqrt(time)*sigma_pos\n",
    "\n",
    "    L1a = torch.sum(np.log(gamma((nu+1)/2.)) - np.log(gamma(nu/2.)) - torch.log(np.sqrt(np.pi*nu)*sigma_tes_t) - (nu+1)/2.*torch.log(1 + 1./nu*(r_test**2/sigma_tes_t**2)))\n",
    "    L1b = torch.sum(np.log(gamma((nu+1)/2.)) - np.log(gamma(nu/2.)) - torch.log(np.sqrt(np.pi*nu)*sigma_pos_t) - (nu+1)/2.*torch.log(1 + 1./nu*(r_pos**2/sigma_pos_t**2)))\n",
    "    L2 = torch.sum((alpha_b-1)*torch.log(X_bar) + (beta_b-1)*torch.log(1-X_bar)) \n",
    "\n",
    "\n",
    "    return -(alpha*(L1a + L1b) + L2)\n",
    "\n",
    "def get_log_like_gradient_and_hessian(V,X,eps=1e-2,compute_hessian=False):\n",
    "    log_pi = V(X)\n",
    "    if compute_hessian:\n",
    "        g = torch.autograd.grad(log_pi,X,retain_graph=True,create_graph=True)[0]\n",
    "        H = torch.stack([torch.autograd.grad(e,X,retain_graph=True)[0] for e in g])\n",
    "        lamda,Q = torch.eig(H,eigenvectors=True)\n",
    "        lamda_prime = torch.sqrt(lamda[:,0]**2 + eps)\n",
    "        lamda_prime_inv = 1./torch.sqrt(lamda[:,0]**2 + eps)\n",
    "        H = Q @ torch.diag(lamda_prime) @ Q.T\n",
    "        Hinv = Q @ torch.diag(lamda_prime_inv) @ Q.T\n",
    "        log_det_Hinv = torch.sum(torch.log(lamda_prime_inv))\n",
    "        return log_pi,g,H,Hinv,log_det_Hinv\n",
    "    else: \n",
    "        return log_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize emulator (a neural network), loss function, and an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Emulator()\n",
    "model.to(device)\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function trains the model, with a higher weight given to models that closely approximate the observed data (we care little if the surrogate works well for poor quality models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,training_,testing_,epochs=200,batch_size=256,weight_factor=50.):\n",
    "    V_vals = torch.stack([V_empirical(x,y.T) for x,y in zip(training_.tensors[0],training_.tensors[1])])\n",
    "\n",
    "    w = torch.exp(-V_vals/weight_factor)\n",
    "    w/=w.sum()\n",
    "\n",
    "\n",
    "    train_weights = TensorDataset(training_.tensors[0],training_.tensors[1],w)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_weights,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=testing_,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=False)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        # Loop over each subset of data\n",
    "        for d,t,w in train_loader:\n",
    "        \n",
    "            t = torch.log2(t+1.)\n",
    "\n",
    "            # Zero out the optimizer's gradient buffer+\n",
    "            optimizer.zero_grad()\n",
    "            # Make a prediction based on the model\n",
    "\n",
    "            outputs = model(d)\n",
    "\n",
    "            # Compute the loss+\n",
    "        \n",
    "            #standardize the output first\n",
    "            loss = 0\n",
    "            residual_squared = (outputs - t)**2\n",
    "            for ww,r in zip(w,residual_squared):\n",
    "                loss += torch.sum(ww*r)\n",
    "                \n",
    "            #print('d',d.min(),d.max())\n",
    "            #print('outputs',outputs.min(),outputs.max())\n",
    "            #print('t',t.min(),t.max())\n",
    "            #print('loss',loss)\n",
    "        \n",
    "            loss.backward()\n",
    "\n",
    "            # Use the derivative information to update the parameters\n",
    "            optimizer.step()\n",
    "    \n",
    "        model.eval()\n",
    "        \n",
    "        if epoch %50 == 0:\n",
    "            training_loss = criterion(model(training_.tensors[0]),torch.log2(training_.tensors[1]+1))\n",
    "            test_loss = criterion(model(testing_.tensors[0]),torch.log2(testing_.tensors[1]+1))\n",
    "            print('finished epoch {}'.format(epoch))\n",
    "            print('loss', training_loss.item(), test_loss.item())\n",
    "            print('*'*50)\n",
    "    \n",
    "    print('finished training') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the MCMC sampling procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_sample(mu,cov,eps=1e-10):\n",
    "    L = torch.cholesky(cov + eps*torch.eye(cov.shape[0],device=device))\n",
    "    return mu + L @ torch.randn(L.shape[0],device=device)\n",
    "\n",
    "def get_proposal_likelihood(Y,mu,inverse_cov,log_det_cov):\n",
    "    return -0.5*log_det_cov - 0.5*(Y - mu) @ inverse_cov @ (Y-mu)\n",
    "\n",
    "def MALA_step(X,h,local_data=None):\n",
    "    if local_data is not None:\n",
    "        pass  \n",
    "    else:\n",
    "        local_data = get_log_like_gradient_and_hessian(V,X,compute_hessian=True)\n",
    "        \n",
    "    log_pi,g,H,Hinv,log_det_Hinv = local_data\n",
    "    \n",
    "    X_ = draw_sample(X,2*h*Hinv).detach()\n",
    "    #X_0[0] = X_0[0].round()\n",
    "    #X_0[1] = X_0[1].round()\n",
    "    #X_0[-1] = X_0[-1].round()\n",
    "    X_.requires_grad=True\n",
    "    \n",
    "    log_pi_ = get_log_like_gradient_and_hessian(V,X_,compute_hessian=False)\n",
    "\n",
    "    logq = get_proposal_likelihood(X_,X,H/(2*h),log_det_Hinv)\n",
    "    logq_ = get_proposal_likelihood(X,X_,H/(2*h),log_det_Hinv)\n",
    "\n",
    "    log_alpha = (-log_pi_ + logq_ + log_pi - logq)\n",
    "    alpha = torch.exp(min(log_alpha,torch.tensor([0.],device=device)))\n",
    "    u = torch.rand(1,device=device)\n",
    "    if u <= alpha and log_alpha!=np.inf:\n",
    "        X.data = X_.data\n",
    "        log_pi_new = get_log_like_gradient_and_hessian(V,X,compute_hessian=False)\n",
    "        local_data[0].data = log_pi_new.data# = get_log_like_gradient_and_hessian(V,X,compute_hessian=True)\n",
    "        s = 1\n",
    "    else:\n",
    "        s = 0\n",
    "    return X,local_data,s,local_data[0]\n",
    "\n",
    "def MALA(X,n_iters=10001,h=0.001,h_max=1.0,acc_target=0.25,k=0.01,beta=0.99,sample_path='./samples/',model_index=0,save_interval=1000,print_interval=50):\n",
    "    print('***********************************************')\n",
    "    print('***********************************************')\n",
    "    print('Running Metropolis-Adjusted Langevin Algorithm for model index {0}'.format(model_index))\n",
    "    print('***********************************************')\n",
    "    print('***********************************************')\n",
    "    local_data = None\n",
    "    vars = []\n",
    "    log_pis = []\n",
    "    acc = acc_target\n",
    "    for i in range(n_iters):\n",
    "        X,local_data,s,log_pi = MALA_step(X,h,local_data=local_data)\n",
    "        vars.append(X.detach())\n",
    "        log_pis.append(log_pi.detach())\n",
    "        acc = beta*acc + (1-beta)*s\n",
    "        h = min(h*(1+k*np.sign(acc - acc_target)),h_max)\n",
    "        if i%print_interval==0:\n",
    "            print('===============================================')\n",
    "            print('sample: {0:d}, acc. rate: {1:4.2f}, log(P): {2:6.1f}'.format(i,acc,local_data[0].item()))\n",
    "            print('curr. m: '+('{:.4f} '*26).format(*X.data.cpu().numpy()))\n",
    "            print('===============================================')\n",
    "          \n",
    "        #if i%save_interval==0:\n",
    "        #    print('///////////////////////////////////////////////')\n",
    "        #    print('Saving samples for model {0:03d}'.format(model_index))\n",
    "        #    print('///////////////////////////////////////////////')\n",
    "        #    X_posterior = torch.stack(vars).cpu().numpy()\n",
    "        #    np.save(open(sample_path+'X_posterior_model_{0:03d}.npy'.format(model_index),'wb'),X_posterior)\n",
    "    X_posterior = torch.stack(vars)#.cpu().numpy()\n",
    "    pi_posterior = torch.stack(log_pis)\n",
    "    return X_posterior,pi_posterior "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that runs the ABM for given parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_abm(run_input):\n",
    "    run_index,parameters = run_input\n",
    "    sample = dict(zip(keys,parameters))\n",
    "    for k in keys_to_round:\n",
    "        sample[k] = int(np.round(sample[k]))\n",
    "\n",
    "    sample['scenario_name'] = 'trial_'+str(run_index)\n",
    "    sample['quarantining'] = 1\n",
    "    sample['social_distancing'] = 1\n",
    "    sample['contact_tracing'] = 1\n",
    "\n",
    "    pandemic = pan.Disease(sample)\n",
    "    pandemic.multiple_runs(5,recorder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate between training the surrogate, running MCMC, and running the ABM with samples from the MCMC sampler to augment the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Number of ABM runs to perform for each iterations\n",
    "n_abm_samples = 20\n",
    "\n",
    "for i in range(0,20):\n",
    "    print('iteration '+str(i))\n",
    "    # Create a new model at each iteration (training is cheap, and this avoids local minima)\n",
    "    model = Emulator()\n",
    "    model.to(device)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    # Train the emulator with the existing training data\n",
    "    train_model(model,training_,testing_,epochs=500)\n",
    "    torch.save(model.state_dict(),'iterative_models/trained_{:03d}.h5'.format(i))\n",
    "    \n",
    "    # Evaluate the surrogate on the training data and find the prediction closest to observation.\n",
    "    # Use this as the initial guess for MCMC sampling\n",
    "    objs = torch.stack([V(X_) for X_ in X])\n",
    "    X_0 = torch.tensor(X[torch.argmin(objs)],requires_grad=True,dtype=torch.float,device=device)\n",
    "    \n",
    "    # Draw 5000 MCMC samples.\n",
    "    X_posterior,pi_posterior = MALA(X_0,n_iters=5001,model_index=0,save_interval=1000,print_interval=100)\n",
    "\n",
    "    # Rescale parameters (the neural net expects them to be z-normalized, but the ABM doesn't)\n",
    "    X_p_numpy = (X_posterior*X_std + X_mean).detach().cpu().numpy().astype(np.float64)\n",
    "    pickle.dump(X_p_numpy,open('mcmc_samples/sample_{:03d}.p'.format(i),'wb'))\n",
    "    \n",
    "    # Initialize a recorder for the ABM\n",
    "    recorder = pan.analysis.recorder(['tests_performed_total', 'positive_tests_total', 'active_cases'], \n",
    "                                     'abm_results/runs_{:03d}.hdf5'.format(i))\n",
    "    \n",
    "    # Draw a few random samples from the MCMC results\n",
    "    samples = X_p_numpy[np.random.choice(range(1000,X_p_numpy.shape[0]),n_abm_samples,replace=False)]\n",
    "    \n",
    "    # Run the ABM on these samples in parallel, then augment the training set with the new ABM runs\n",
    "    try:\n",
    "        Parallel(n_jobs=4)(delayed(run_abm)(x) for x in enumerate(samples))\n",
    "        newset = hdf5Dataset_init('abm_results/runs_{:03d}.hdf5'.format(i))   \n",
    "        train_new, test_new = newset.split_datasets(.1,split_type='parameter')\n",
    "        train_new_,test_new_,_,_ = normalize_sets(train_new,test_new,X_mean=X_mean,X_std=X_std)\n",
    "        training_ = TensorDataset(torch.cat((training_.tensors[0],train_new_.tensors[0])),torch.cat((training_.tensors[1],train_new_.tensors[1])))\n",
    "        testing_ = TensorDataset(torch.cat((testing_.tensors[0],test_new_.tensors[0])),torch.cat((testing_.tensors[1],test_new_.tensors[1])))\n",
    "\n",
    "        X = torch.stack([t[0] for t in training_])\n",
    "        X_min = X.cpu().numpy().min(axis=0)-1e-3\n",
    "        X_max = X.cpu().numpy().max(axis=0)+1e-3\n",
    "        X_min = torch.tensor(X_min,dtype=torch.float32,device=device)\n",
    "        X_max = torch.tensor(X_max,dtype=torch.float32,device=device)\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model that was trained most recently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('iterative_models/trained_019.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform some plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(nrows=3,figsize=(12,12))\n",
    "\n",
    "# This is the best model the surrogate produces\n",
    "X_star = X_posterior[np.argmin(pi_posterior.cpu().numpy())]\n",
    "pred_star = 2**model(X_star) - 1\n",
    "\n",
    "# These are the posterior log probabilities for each training example\n",
    "V_vals = torch.stack([V_empirical(x,y.T) for x,y in zip(training_.tensors[0],training_.tensors[1])])\n",
    "\n",
    "# Plot a bunch of predictions from the MCMC samples\n",
    "for i in range(0,5000,20):\n",
    "    index = i# np.random.randint(1000)\n",
    "    pred_opt = 2**model(X_posterior[index]) - 1.\n",
    "    axs[0].plot(pred_opt[:n_data,2].detach().cpu().numpy(),'k-',alpha=0.02,rasterized=True)\n",
    "    axs[1].plot(pred_opt[:n_data,1].detach().cpu().numpy(),'k-',alpha=0.02,rasterized=True)\n",
    "    axs[2].plot(pred_opt[:n_data,0].detach().cpu().numpy(),'k-',alpha=0.02,rasterized=True)\n",
    "    #axs[2].plot(X_posterior[index,*pred_opt[:n_data,0].detach().cpu().numpy(),'b--',alpha=0.02,rasterized=True)\n",
    "\n",
    "# Plot the data\n",
    "axs[0].plot(test_obs[:n_data].detach().cpu().numpy(),'r:',lw=3.0)\n",
    "axs[1].plot(pos_obs[:n_data].detach().cpu().numpy(),'r:',lw=3.0)\n",
    "axs[0].plot(pred_star[:n_data,2].detach().cpu().numpy(),'b--',lw=3.0)\n",
    "axs[1].plot(pred_star[:n_data,1].detach().cpu().numpy(),'b--',lw=3.0)\n",
    "axs[2].plot(pred_star[:n_data,0].detach().cpu().numpy(),'b--',lw=3.0)\n",
    "#axs[2].plot(pred_star[:n_data,0].detach().cpu().numpy(),'b--',lw=3.0)\n",
    "\n",
    "# Plot the best fitting (n) training example(s).\n",
    "for idx in torch.sort(V_vals)[1][0:1]:\n",
    "    best_train = training_.tensors[1][idx].detach().cpu().numpy()\n",
    "    pred_hat = 2**model(training_.tensors[0][idx]).detach().cpu().numpy()\n",
    "    axs[0].plot(best_train[2,:n_data],'g-',alpha=0.5)\n",
    "    axs[1].plot(best_train[1,:n_data],'g-',alpha=0.5)\n",
    "    axs[2].plot(best_train[0,:n_data],'g-',alpha=0.5)\n",
    "    axs[0].plot(pred_hat[:n_data,2],'g--',alpha=0.5)\n",
    "    axs[1].plot(pred_hat[:n_data,1],'g--',alpha=0.5)\n",
    "    axs[2].plot(pred_hat[:n_data,0],'g--',alpha=0.5)\n",
    "\n",
    "axs[2].plot(ac[:,0],ac[:,1],'r:',lw=3.0)\n",
    "axs[2].set_xlim(0,n_data)\n",
    "axs[1].set_xlim(0,n_data)\n",
    "axs[0].set_xlim(0,n_data)\n",
    "axs[2].set_ylim(0,1500)\n",
    "\n",
    "axs[0].set_ylabel('# Tests')\n",
    "axs[1].set_ylabel('# Positive')\n",
    "axs[2].set_ylabel('Active Cases')\n",
    "axs[2].set_xlabel('Days since 08/19/2020')\n",
    "\n",
    "fig.savefig('Covid_surrogate_ModelvsObs.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print a dictionary of the mean and map parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ['initial_infected_fraction', 'initial_removed_fraction', 'incubation_period', 'serial_interval', 'symptomatic_fraction', 'recovery_days', 'quarantine_days', 'days_indetectable', 'R0', 'contact_rate', 'npi_factor', 'contact_tracing_testing_rate', 'contact_tracing_quarantine_rate', 'contact_tracing_days', 'daily_testing_fraction', 'daily_testing_false_positive', 'daily_testing_false_negative', 'class_size_limit', 'contact_upscale_factor', 'friendship_contacts', 'academic_contacts', 'broad_social_contacts', 'department_environmental_contacts', 'broad_environmental_contacts', 'residential_neighbors', 'online_transition']\n",
    "X_p_numpy = (X_posterior*X_std + X_mean).detach().cpu().numpy().astype(np.float64)\n",
    "X_ = training_.tensors[0][idx]\n",
    "X__numpy = (X_*X_std + X_mean).detach().cpu().numpy()\n",
    "\n",
    "mean_params = dict(zip(keys,X_p_numpy.mean(axis=0)))\n",
    "map_train = dict(zip(keys,X__numpy))\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(mean_params)\n",
    "pprint.pprint(map_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
